{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Resonance Model Experiment\n",
    "\n",
    "## Introduction\n",
    "\n",
    "As explained in the paper, this data comes from **The Big Bad NLP Database** (https://datasets.quantumstat.com). We chose the AG News set, which has about 120k articles for topic classification - the topics being world, sports, business, and sci/tech. We also chose the Amazon Fine Food Reviews dataset to introduce text that we expect would be different than any of the text found in AG, in particular those articles classified as sports articles.\n",
    "\n",
    "The idea for the experiment is as such:\n",
    " - Gather only sports articles from AG\n",
    " - Conduct a 60/40 split over these sports articles to get the baseline corpus $B$ and the first target corpus $T_1$, respectively.\n",
    " - Randomly gather a sample from the Amazon Fine Food Reviews (we sample the same number of rows as $T_1$) to get the second target corpus, $T_2$\n",
    " - Train word embeddings over the three corpora, and compare resonance scores derived from the word embeddings between $B$ and $\\{T_1, T_2\\}$\n",
    " \n",
    "Our hypothesis is that the resonance score will be higher for $T_1$ than that of $T_2$.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "We felt the need to scale down since we are running this experiment on a mere 2015 Mac...not Google's Sycamore...hence, after loading in the total datasets, we do some analysis to determine an efficient way to cut out rows. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AG DATA  : keys:  ['Category', 'Description', 'Text']  length:  120000 \n",
      "\n",
      "FOOD DATA: keys:  ['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text']  length:  568454\n"
     ]
    }
   ],
   "source": [
    "def loadData(f_name, col_names = None):\n",
    "    if col_names is None:\n",
    "        return pd.read_csv(f_name)\n",
    "    else:\n",
    "        return pd.read_csv(f_name, names = col_names, header = None)\n",
    "\n",
    "ag   = loadData('./data/ag_train.csv', col_names=['Category', 'Description', 'Text'])\n",
    "food = loadData('./data/food_reviews.csv', )\n",
    "print('AG DATA  : keys: ', list(ag.keys()), ' length: ', len(ag), '\\n')\n",
    "print('FOOD DATA: keys: ', list(food.keys()), ' length: ', len(food))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting AG data in shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sports category is 2 for AG so we will extract only those datapoints\n",
    "\n",
    "ag_sports  = copy.deepcopy(ag.loc[ag['Category'] == 2])\n",
    "ag_sports.drop(['Category', 'Description'], axis=1, inplace=True)\n",
    "\n",
    "indices    = list(np.arange(0, len(ag_sports)))\n",
    "train_ind  = random.sample(indices, round(0.6*len(indices)))\n",
    "target_ind = list(set(indices) - set(train_ind))\n",
    "\n",
    "assert(len(train_ind)+len(target_ind) == len(indices))\n",
    "\n",
    "baseline_1 = copy.deepcopy(ag_sports.iloc[train_ind])\n",
    "t1_1       = copy.deepcopy(ag_sports.iloc[target_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that many of these data have the news source and/or location as a prefix to the article text, such as: <br>\n",
    "- AP - Darin Erstad doubled in the go-ahead run in the eighth inning, lifting the Anaheim Angels to a 3-2 victory over the Detroit Tigers on Sunday. The win pulled Anaheim within a percentage point of Boston and Texas in the AL wild-card race.\n",
    "- MILWAUKEE (Sports Network) - U.S. Ryder Cup captain Hal  Sutton finalized his team on Monday when he announced the  selections of Jay Haas and Stewart Cink as his captain's picks.\n",
    "- ATHENS (Reuters) - At the beach volleyball, the 2004  Olympics is a sell-out, foot-stomping success.\n",
    "- HAVEN, Wis. -- Perched high on the bluffs overlooking Lake Michigan, Whistling Straits is a massive, windswept landscape, as large a golf course as \\$40 million can buy. It is complete with sand dunes that could double as ski slopes and deep bunkers that should require elevators.\n",
    "\n",
    "However, others do not. We would like to get rid of these sources as they can inject our model with information that indicates $B$ and $T_1$ were pulled from the same set since the Amazon data will not come with these prefixes.\n",
    "\n",
    "We cleaned this by splitting the string on the '-' or '--' character, deleting the first element of the list (the news source/location prefix), and then rejoining the remaining list to get the actual article. We note here that this is perhaps not the most efficient or elegant means of cleaning, since a '-' character could exist within the text without a prefix, and hence the article would be chopped off, but we take this as a small loss compared to the amount that will be clean. To get rid of rows that are extremely messed up by this method, we only keep those rows with more than 5 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAG(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if '--' in row['Text']:\n",
    "            clean_text  = removePrefix(row['Text'], '--')\n",
    "            row['Text'] = clean_text\n",
    "        elif '-' in row['Text']:\n",
    "            clean_text  = removePrefix(row['Text'], '-')\n",
    "            row['Text'] = clean_text\n",
    "    return df\n",
    "\n",
    "def removePrefix(text, char):\n",
    "    temp = text.split(char)\n",
    "    del temp[0]\n",
    "    return (char).join(temp)\n",
    "\n",
    "#def removeDoubleDash(pd_series):\n",
    "#    return pd_series.str.split('--', expand = True)[1]\n",
    "\n",
    "baseline_2 = cleanAG(baseline_1)\n",
    "t1_2       = cleanAG(t1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Rows in B :  17155\n",
      "Num Rows in T1:  11469\n"
     ]
    }
   ],
   "source": [
    "baseline_3 = copy.deepcopy(baseline_2[baseline_2['Text'].str.split().str.len() > 5])\n",
    "print('Num Rows in B : ', len(baseline_3))\n",
    "t1_3       = copy.deepcopy(t1_2[t1_2['Text'].str.split().str.len() > 5])\n",
    "print('Num Rows in T1: ', len(t1_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Amazon Review data in shape\n",
    "\n",
    "This was simple because the text is pretty clean already - we just extract a random sample of the same size of $T_1$ from the ~570k rows available, and then extracted only the text for our $T_2$ corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Rows in T2:  11469\n"
     ]
    }
   ],
   "source": [
    "food_indices = np.random.randint(0, high = len(food), size = len(t1_3))\n",
    "t2_1 = food.iloc[food_indices]\n",
    "t2_2 = pd.DataFrame(t2_1['Text'])\n",
    "print('Num Rows in T2: ', len(t2_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Get To Some Word Embeddings\n",
    "\n",
    "Even though it is probably best practice to clean up our text a little bit more (perhaps lemmatizing, getting rid of stop words, changing all tokens to lowercase, ridding our text of punctuation, etc.), I read in *Natural Language Processing in Action* (Lane et al.) that cleaning further may actually rid our rich dataset of valuable information. For example, when I told this to a colleague, he pondered for a minute then recalled a paper he read that was able to identify racist terminology by the way the word 'the' was used. In racist text, 'the' typically is used just before a demeaning word/phrase. Perhaps this was a key indicator for this model, although 'the' is listed as a stopword almost everywhere.\n",
    "\n",
    "So, without further ado, let's get to the meat and potatos of this notebook and start training with some word embeddings. In the book mentioned above, it says 'The gensimword2vec model expects a list of sentences, where each sentence is broken up into tokens'. Hence, after loading in Word2Vec, I transform the data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [value.split() for index, value in baseline_3['Text'].items()]\n",
    "t1       = [value.split() for index, value in t1_3['Text'].items()]\n",
    "t2       = [value.split() for index, value in t2_2['Text'].items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will set some parameters for training our Word2Vec, but first I want to address the importance of **setting a seed so that the models for each corpora will initialize the same way**. This is key - otherwise we would have no consistency since initialization of the weights in a neural net is a random process. However, after doing some research on the almighty StackExchange, I found that Word2Vec does this anyway...cheeky geniuses. To be clear, if I train on the same Jupyter Notebook kernel, the initialization will be consistent. However, if I restart and clear the kernel, I will get different results. For our purpose, this works just fine because I certainly will not be clearing the kernel in between running cells?? Can you even do that? Let's verify here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = baseline[:10]\n",
    "\n",
    "toymodel1 = Word2Vec(\n",
    "    test_sentences,\n",
    "    workers   = 1,\n",
    "    size      = 32,\n",
    "    min_count = 1,\n",
    "    window    = 4,\n",
    "    sample    = 0.001)\n",
    "toymodel2 = Word2Vec(\n",
    "    test_sentences,\n",
    "    workers   = 1,\n",
    "    size      = 32,\n",
    "    min_count = 1,\n",
    "    window    = 4,\n",
    "    sample    = 0.001)\n",
    "\n",
    "toymodel1_dict = {key:toymodel1.wv[key] for idx, key in enumerate(toymodel1.wv.vocab)}\n",
    "toymodel2_dict = {key:toymodel2.wv[key] for idx, key in enumerate(toymodel2.wv.vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(toymodel1_dict['the'] == toymodel2_dict['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = list(toymodel1_dict.keys())\n",
    "k2 = list(toymodel2_dict.keys())\n",
    "assert(k1==k2)\n",
    "for word in k1:\n",
    "    if not np.array_equal(toymodel1_dict[word], toymodel2_dict[word]):\n",
    "        print('Embeddings are Different For: ', word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, enough with the build-up...let's train word embeddings on each corpus. \n",
    "- The vector length is set to 128; it's relatively small because the corpora are not huge.\n",
    "- The number or workers is set to 1 as the documentation states that this eliminates \"order jitter' on the computer (so we ensure we start with the same initial NN weights).\n",
    "- The window size is small as well since our sentences are not very long.\n",
    "- The subsampling rate is the 'threshold for configuring which higher-frequency words are randomly downsampled', and we set it to the recommended value.\n",
    "- As stated in *Natural Language Processing In Action*, the 'skip-gram approach works well with small corpora and rare terms', but I still choose CBOW because it 'shows higher accuracies for frequent words and is much faster to train'.\n",
    "- To further ensure quality embeddings, we increase the default number of epochs from 5 to 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWordVecs(params, corpus):\n",
    "    model = Word2Vec(\n",
    "            corpus,\n",
    "            size      = params['num_features'],\n",
    "            min_count = params['min_word_count'],\n",
    "            workers   = params['num_workers'],\n",
    "            window    = params['window_size'],\n",
    "            sample    = params['sample_rate'],\n",
    "            sg        = params['skipgram'],\n",
    "            iter      = params['epochs'])\n",
    "    return {key:model.wv[key] for idx, key in enumerate(model.wv.vocab)}\n",
    "\n",
    "params = {\n",
    "'num_features': 128,\n",
    "'min_word_count': 3,\n",
    "'num_workers': 1,\n",
    "'window_size': 2,\n",
    "'sample_rate': 0.00001,\n",
    "'skipgram': 0,\n",
    "'epochs': 20}\n",
    "\n",
    "baseline_dict = trainWordVecs(params, baseline)\n",
    "t1_dict       = trainWordVecs(params, t1)\n",
    "t2_dict       = trainWordVecs(params, t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, now we find the words that are found in all three corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed 12937 word embeddings in baseline_mod.\n",
      "Computed 9996 word embeddings in t1_mod.\n",
      "Computed 16471 word embeddings in t2_mod.\n",
      "Found 3304 common words.\n"
     ]
    }
   ],
   "source": [
    "baseline_words = set(baseline_dict.keys())\n",
    "t1_words       = set(t1_dict.keys())\n",
    "t2_words       = set(t2_dict.keys())\n",
    "\n",
    "print('Computed {} word embeddings in baseline_mod.'.format(len(baseline_words)))\n",
    "print('Computed {} word embeddings in t1_mod.'.format(len(t1_words)))\n",
    "print('Computed {} word embeddings in t2_mod.'.format(len(t2_words)))\n",
    "\n",
    "common_words = list(baseline_words & t1_words & t2_words)\n",
    "print('Found {} common words.'.format(len(common_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Finally, after training, we need to compute the distances between the embeddings of the words found in the list $common\\_words$. We do so for all of the methods mentioned in the paper, and show the final results at the very bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResults(input_dict):\n",
    "    metric = input_dict['distance_metric']\n",
    "    lamb = input_dict['lambda']\n",
    "    common_words = input_dict['common_words']\n",
    "    baseline_dict = input_dict['baseline_dict']\n",
    "    t1_dict = input_dict['t1_dict']\n",
    "    t2_dict = input_dict['t2_dict']\n",
    "    t1_distances = []\n",
    "    t2_distances = []\n",
    "    for word in input_dict['common_words']:\n",
    "        base_vec = getWordVec(baseline_dict, word)\n",
    "        t1_vec   = getWordVec(t1_dict, word)\n",
    "        t2_vec   = getWordVec(t2_dict, word)\n",
    "        dist1    = getEmbeddingDistance(base_vec, t1_vec, metric)\n",
    "        dist2    = getEmbeddingDistance(base_vec, t2_vec, metric)\n",
    "        t1_distances.append(dist1)\n",
    "        t2_distances.append(dist2)\n",
    "    t1_resonance = computeResonance(np.array(t1_distances), lamb, metric)\n",
    "    t2_resonance = computeResonance(np.array(t2_distances), lamb, metric)\n",
    "    if input_dict['log_res']:\n",
    "        saveResults(common_words, t1_distances, t2_distances)\n",
    "    return t1_resonance, t2_resonance\n",
    "\n",
    "def getWordVec(word_dict, word):\n",
    "    return np.array(word_dict[word])\n",
    "\n",
    "def getEmbeddingDistance(baseline_vec, target_vec, distance_metric):\n",
    "    if distance_metric == 'euclidean':\n",
    "        return np.linalg.norm((baseline_vec - target_vec))\n",
    "    elif distance_metric == 'manhattan':\n",
    "        return np.sum(abs(baseline_vec - target_vec))\n",
    "    elif distance_metric == 'cosine_sim_neg':\n",
    "        return np.dot(baseline_vec, target_vec)/(np.linalg.norm(baseline_vec)*np.linalg.norm(target_vec))\n",
    "    elif distance_metric == 'cosine_sim_pos':\n",
    "        return max(0, np.dot(baseline_vec, target_vec)/(np.linalg.norm(baseline_vec)*np.linalg.norm(target_vec)))\n",
    "    \n",
    "def computeResonance(distance_array, lamb, metric):\n",
    "    if metric == 'euclidean' or metric == 'manhattan':\n",
    "        return 100 - 100*(np.tanh((1/lamb)*np.sum(distance_array)))\n",
    "    elif metric == 'cosine_sim_neg':\n",
    "        return 100/(1 + np.exp((-1/lamb)*np.sum(distance_array)))\n",
    "    elif metric == 'cosine_sim_pos':\n",
    "        return 100*(np.tanh((1/lamb)*np.sum(distance_array)))\n",
    "    \n",
    "def saveResults(common_words, t1_distances, t2_distances):\n",
    "    df = pd.DataFrame(list(zip(common_words, t1_distances, t2_distances)),\n",
    "                      columns =['word', 't1_distance', 't2_distance'])\n",
    "    df['t1>t2'] = np.where(df['t1_distance'] >= df['t2_distance'], 1, 0)\n",
    "    df.to_csv('results/{}.csv'.format(datetime.datetime.now().strftime('%Y_%m_%d_%f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclid = {'distance_metric': 'euclidean',\n",
    "          'lambda':10000,\n",
    "          'common_words': common_words,\n",
    "          'baseline_dict': baseline_dict,\n",
    "          't1_dict': t1_dict,\n",
    "          't2_dict': t2_dict,\n",
    "          'log_res': False}\n",
    "manh = {'distance_metric': 'manhattan',\n",
    "          'lambda':100000,\n",
    "          'common_words': common_words,\n",
    "          'baseline_dict': baseline_dict,\n",
    "          't1_dict': t1_dict,\n",
    "          't2_dict': t2_dict,\n",
    "          'log_res': False}\n",
    "cos_neg = {'distance_metric': 'cosine_sim_neg',\n",
    "          'lambda':2500,\n",
    "          'common_words': common_words,\n",
    "          'baseline_dict': baseline_dict,\n",
    "          't1_dict': t1_dict,\n",
    "          't2_dict': t2_dict,\n",
    "          'log_res': False}\n",
    "cos_pos = {'distance_metric': 'cosine_sim_pos',\n",
    "          'lambda':2700,\n",
    "          'common_words': common_words,\n",
    "          'baseline_dict': baseline_dict,\n",
    "          't1_dict': t1_dict,\n",
    "          't2_dict': t2_dict,\n",
    "          'log_res': False}\n",
    "\n",
    "t1_euclid, t2_euclid = getResults(euclid)\n",
    "t1_manh, t2_manh = getResults(manh)\n",
    "t1_cn, t2_cn = getResults(cos_neg)\n",
    "t1_cp, t2_cp = getResults(cos_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean : T1 Resonance:  70.54  T2 Resonance:  54.7\n",
      "Manhattan : T1 Resonance:  72.84  T2 Resonance:  58.38\n",
      "Cosine Neg: T1 Resonance:  73.73  T2 Resonance:  64.48\n",
      "Cosine Pos: T1 Resonance:  74.23  T2 Resonance:  50.22\n"
     ]
    }
   ],
   "source": [
    "print('Euclidean : T1 Resonance: ', round(t1_euclid,2), ' T2 Resonance: ', round(t2_euclid,2))\n",
    "print('Manhattan : T1 Resonance: ', round(t1_manh,2), ' T2 Resonance: ', round(t2_manh,2))\n",
    "print('Cosine Neg: T1 Resonance: ', round(t1_cn,2), ' T2 Resonance: ', round(t2_cn,2))\n",
    "print('Cosine Pos: T1 Resonance: ', round(t1_cp,2), ' T2 Resonance: ', round(t2_cp,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
